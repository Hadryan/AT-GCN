from scipy import stats
import numpy
from sklearn import metrics

def roc(pred, truth):
    data = numpy.array(sorted(zip(pred, truth), reverse = True))
    pred, truth = data[:,0], data[:,1]
    TP = truth.cumsum()
    FP = (1 - truth).cumsum()
    mask = numpy.concatenate([numpy.diff(pred) < 0, numpy.array([True])])
    TP = numpy.concatenate([numpy.array([0]), TP[mask]])
    FP = numpy.concatenate([numpy.array([0]), FP[mask]])
    return TP, FP

def ap_and_auc(pred, truth):
    TP, FP = roc(pred, truth)
    auc = ((TP[1:] + TP[:-1]) * numpy.diff(FP)).sum() / (2 * TP[-1] * FP[-1])
    precision = TP[1:] / (TP + FP)[1:]
    weight = numpy.diff(TP)
    ap = (precision * weight).sum() / TP[-1]
    return ap, auc

def dprime(auc):
    return stats.norm().ppf(auc) * numpy.sqrt(2.0)

def gas_eval(pred, truth):
    if truth.ndim == 1:
        ap, auc = ap_and_auc(pred, truth)
    else:
        ap, auc = numpy.array([ap_and_auc(pred[:,i], truth[:,i]) for i in range(truth.shape[1]) if truth[:,i].any()]).mean(axis = 0)
    return ap, auc, dprime(auc)

def gas_eval_sub(pred, truth):
    if truth.ndim == 1:
        ap, auc = ap_and_auc(pred, truth)
        b, c = ap, auc
    else:
        ap, auc = numpy.array([ap_and_auc(pred[:,i], truth[:,i]) for i in range(truth.shape[1]) if truth[:,i].any()]).mean(axis = 0)
        x = numpy.array([ap_and_auc(pred[:,i], truth[:,i]) for i in range(truth.shape[1]) if truth[:,i].any()])
        b,c = x[:,0], x[:,1]
        
    return ap, auc, dprime(auc),b,c

def dcase_sed_eval(outputs, pooling, thres, truth, seg_len, verbose = False):
    pred = outputs[1].reshape((-1, seg_len, outputs[1].shape[-1]))
    if pooling == 'max':
        seg_prob = pred.max(axis = 1)
    elif pooling == 'ave':
        seg_prob = pred.mean(axis = 1)
    elif pooling == 'lin':
        seg_prob = (pred * pred).sum(axis = 1) / pred.sum(axis = 1)
    elif pooling == 'exp':
        seg_prob = (pred * numpy.exp(pred)).sum(axis = 1) / numpy.exp(pred).sum(axis = 1)
    elif pooling == 'att' or pooling == 'att_nn' or pooling == 'att_gated' or pooling == 'multi_head_att' or pooling == 'att_smoothing' or pooling == 'att_nn_shape':
        att = outputs[2].reshape((-1, seg_len, outputs[2].shape[-1]))
        seg_prob = (pred * att).sum(axis = 1) / att.sum(axis = 1)

    pred = seg_prob >= thres
    truth = truth.reshape((-1, seg_len, truth.shape[-1])).max(axis = 1)

    if not verbose:
        Ntrue = truth.sum(axis = 1)
        Npred = pred.sum(axis = 1)
        Ncorr = (truth & pred).sum(axis = 1)
        Nmiss = Ntrue - Ncorr
        Nfa = Npred - Ncorr

        error_rate = 1.0 * numpy.maximum(Nmiss, Nfa).sum() / Ntrue.sum()
        f1 = 2.0 * Ncorr.sum() / (Ntrue + Npred).sum()
        return error_rate, f1
    else:
        class Object(object):
            pass
        res = Object()
        res.TP = (truth & pred).sum()
        res.FN = (truth & ~pred).sum()
        res.FP = (~truth & pred).sum()
        res.precision = 100.0 * res.TP / (res.TP + res.FP)
        res.recall = 100.0 * res.TP / (res.TP + res.FN)
        res.F1 = 200.0 * res.TP / (2 * res.TP + res.FP + res.FN)
        res.sub = numpy.minimum((truth & ~pred).sum(axis = 1), (~truth & pred).sum(axis = 1)).sum()
        res.dele = res.FN - res.sub
        res.ins = res.FP - res.sub
        res.ER = 100.0 * (res.sub + res.dele + res.ins) / (res.TP + res.FN)
        return res

def calculate_stats(output, target):
    classes_num = target.shape[-1]
    stats = []
    # Class-wise statistics
    for k in range(classes_num):

        # Average precision
        avg_precision = metrics.average_precision_score(
            target[:, k], output[:, k], average=None)

        # AUC
        auc = metrics.roc_auc_score(target[:, k], output[:, k], average=None)

        # Precisions, recalls
        (precisions, recalls, thresholds) = metrics.precision_recall_curve(
            target[:, k], output[:, k])

        # FPR, TPR
        (fpr, tpr, thresholds) = metrics.roc_curve(target[:, k], output[:, k])

        save_every_steps = 1000     # Sample statistics to reduce size
        dict = {'precisions': precisions[0::save_every_steps],
                'recalls': recalls[0::save_every_steps],
                'AP': avg_precision,
                'fpr': fpr[0::save_every_steps],
                'fnr': 1. - tpr[0::save_every_steps],
                'auc': auc}
        stats.append(dict)

    return stats

def gas_eval_kong(pred, truth):
    print(pred.shape)
    print(truth.shape)
    print(truth[:,i].shape)
    stats = calculate_stats(pred, truth)
    mAP = np.mean([stat['AP'] for stat in stats])
    mAUC = np.mean([stat['auc'] for stat in stats])
    return mAP, mAUC, dprime(auc) 
